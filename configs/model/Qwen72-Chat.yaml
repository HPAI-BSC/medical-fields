# https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/llm.py
name: Qwen-72B-Chat
vllm:
  model: "/gpfs/projects/bsc70/heka/models/${model.name}"
  tokenizer: "/gpfs/projects/bsc70/heka/models/${model.name}"
  tensor_parallel_size: 4
  trust_remote_code: True
  max_model_len: 2048
  gpu_memory_utilization: 0.75 

sampling_params:
  temperature: 0.1
  top_p: 0.9
  max_tokens: 5
  stop: ["<|im_end|>", "<|endoftext|>", "<|im_start|>"]